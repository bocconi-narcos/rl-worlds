# 02 Data Collection

## The Importance of High-Quality Datasets

The performance and generalization capabilities of world models are critically dependent on the quality and diversity of the data they are trained on. For a model to learn meaningful representations of an environment's dynamics, it must be exposed to a wide range of states, transitions, and interactions. Insufficiently diverse data can lead to overfitting, where the model performs well on familiar scenarios but fails to generalize to new, unseen situations. Conversely, a rich dataset encompassing varied trajectories allows the model to learn robust and broadly applicable representations, which are essential for effective downstream tasks such as planning and policy learning in reinforcement learning.

## Data Collection Pipeline

Our data collection process is managed by scripts within the `src/` directory, primarily `src/data_handling.py`, which orchestrates the collection and preparation of datasets, and `src/utils/data_utils.py`, which contains the core logic for interacting with Gymnasium environments and processing the collected data.

The pipeline can be configured to collect data through two primary methods:
1.  **Guided Exploration using a Pre-trained PPO Agent:** Leveraging an agent trained via Proximal Policy Optimization (PPO).
2.  **Random Action Sampling:** Taking random actions within the environment's action space.

Collected experiences, consisting of (state, action, reward, next_state) tuples, are processed (e.g., image resizing and normalization) and then structured into PyTorch `Dataset` and `DataLoader` objects for efficient training of the world models.

### Guided Exploration with a PPO Agent

To generate trajectories that are both diverse and relevant to potential downstream tasks, we employ a pre-trained PPO agent, defined in `src/rl_agent.py`. This agent is first trained for a specified number of timesteps (controlled by `ppo_agent.total_train_timesteps` in `config.yaml`) within the target Gymnasium environment. Once trained, this agent is used to navigate the environment and collect experience data.

The rationale for using a PPO agent for data collection, as opposed to purely random exploration, is multifaceted:
-   **Task-Relevant Trajectories:** A trained agent is more likely to explore states and transitions that are meaningful for solving tasks within the environment. This can lead to a dataset that is richer in useful information for the world model.
-   **Deeper Exploration:** An agent trained to achieve certain goals (even if sub-optimally) can often reach parts of the state space that random exploration might seldom encounter.
-   **Structured Behavior:** The trajectories generated by an agent exhibit more coherent and structured sequences of actions and states compared to random noise, potentially aiding the world model in learning complex dynamics.

The PPO agent's parameters, such as `learning_rate`, `n_steps`, `batch_size`, `policy_type` (e.g., `CnnPolicy` for image-based environments), etc., are configured under the `ppo_agent` block in `config.yaml`. The `ppo_agent.enabled` flag must be set to `true` to use this data collection method.

### Random Action Sampling

As an alternative, or for baseline comparisons, data can be collected by taking random actions at each step in the environment. This method is simpler and does not require pre-training an RL agent. While it can provide broad coverage of the state space, it may not efficiently explore task-relevant regions or generate trajectories with complex, meaningful behaviors. This mode is activated if `ppo_agent.enabled` is `false` in `config.yaml`.

The script `src/utils/data_utils.py` contains the function `collect_random_episodes` which handles this mode of data collection.

## Dataset Management: Saving, Loading, and Configuration

The collected datasets are managed through functionalities within `src/utils/data_utils.py` and configured via `config.yaml`.

-   **Saving Datasets:** After data collection (either random or PPO-guided), the resulting training and validation datasets (instances of `ExperienceDataset`) are saved to disk as a pickle file. The directory for saving is specified by `dataset_dir` in `config.yaml`, and the filename is determined by `dataset_filename`. Metadata about the collection process (e.g., environment name, number of episodes, collection method) is also saved alongside the datasets.

-   **Loading Datasets:** To reuse previously collected data and avoid redundant collection, the system can load datasets from disk. If `load_dataset_path` in `config.yaml` is set to a valid filename (relative to `dataset_dir`), the system will attempt to load this file. This allows for reproducibility and faster iteration during model development. The loading logic also checks for environment name mismatches between the loaded dataset and the current configuration.

-   **Configuration Parameters:** Key settings in `config.yaml` related to data collection include:
    -   `environment_name`: Specifies the Gymnasium environment to use.
    -   `num_episodes_data_collection`: The total number of episodes to collect.
    -   `max_steps_per_episode_data_collection`: The maximum number of steps to run within each episode during collection.
    -   `dataset_dir`: The directory where datasets are stored.
    -   `load_dataset_path`: Path to a dataset file to load (if any). If empty, new data is collected.
    -   `dataset_filename`: The name of the file to save newly collected data.
    -   `ppo_agent`: A block containing configurations for the PPO agent if PPO-guided exploration is enabled (see above).
    -   `validation_split` (within `early_stopping` block but used by data utility): The proportion of collected episodes to be set aside for the validation set.

This structured approach to data collection and management ensures that our world models are trained on datasets that are appropriate for the research objectives, and that the process is configurable and reproducible.
