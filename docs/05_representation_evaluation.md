# 05 Representation Evaluation

## 1. Introduction to Representation Evaluation

Evaluating the quality of learned representations is paramount in the study of world models. The internal representations, or embeddings, generated by these models are intended to encapsulate the essential information about the environment's state and dynamics. For research aimed at top-tier AI conferences, a rigorous assessment is necessary to determine if these representations are not merely compressive but also capture meaningful, actionable, and task-relevant information. This section details the methodologies employed to assess and compare the representations learned by both the Standard Encoder-Decoder and the Joint Embedding Predictive Architecture (JEPA) models.

## 2. Reward Prediction Task

A primary method for evaluating the utility of learned representations is to assess their effectiveness in predicting task-relevant variables, such as environmental rewards. If a representation can be used to accurately predict rewards, it suggests that it has captured features of the state that are salient for understanding progress and success within the environment, a crucial aspect for downstream reinforcement learning tasks.

### Concept

The core idea is to train a lightweight predictive model—specifically, a Multi-Layer Perceptron (MLP)—on top of the *frozen* latent representations produced by a pre-trained world model. This MLP, termed `RewardPredictorMLP`, attempts to predict the scalar reward `r_t` associated with the state (or state-action pair) from which the representation was derived.

### Implementation Details

-   **Model:** The `RewardPredictorMLP`, defined in `src/models/mlp.py`, is a simple MLP architecture. Its structure (number of hidden layers, dimensions, activation functions, batch normalization) is configurable.
-   **Input Representations:**
    -   For the **Standard Encoder-Decoder model**, the input to the `RewardPredictorMLP` is the latent representation `z_t` produced by its encoder (e.g., ViT, CNN, or MLP encoder) from the current state `s_t`.
    -   For the **JEPA model**, the input is typically the representation generated by its *online encoder* for the current state `s_t` (i.e., `online_encoded_s_t`). Using the online encoder's output reflects the most current understanding of the state by the learnable part of JEPA.
    The world model (Encoder-Decoder or JEPA) is pre-trained and its weights are **frozen** during the training of the `RewardPredictorMLP`. This ensures that the evaluation truly probes the fixed qualities of the learned representation.
-   **Prediction Target:** The `RewardPredictorMLP` is trained to predict the scalar reward `r_t` that was received from the environment when the agent was in state `s_t` (or transitioned from `s_t` by taking `a_t`). The `ExperienceDataset` (from `src/utils/data_utils.py`) provides these (state, action, reward, next_state) tuples. As seen in `tests/test_reward_prediction.py`, the reward `r` is a scalar tensor.
-   **Loss Function:** The `RewardPredictorMLP` is trained using a Mean Squared Error (MSE) loss if rewards are continuous, or potentially a cross-entropy loss if rewards are discrete and framed as a classification problem (though MSE for scalar rewards is typical).
    `L_reward_pred = MSE(MLP(frozen_representation_s_t), r_t)`

### Interpretation

The performance of the `RewardPredictorMLP` (e.g., low validation MSE) is indicative of the quality of the underlying world model's representations. If the MLP can accurately predict rewards using these representations, it implies that the representations have successfully encoded information critical to understanding the environment's reward structure. This is a strong positive signal for the utility of these representations in subsequent reinforcement learning agents that rely on reward signals.

### Configuration

The `config.yaml` file contains a `reward_predictors` block to manage this evaluation task:
```yaml
reward_predictors:
  encoder_decoder_reward_mlp:
    enabled: true
    # input_type: "flatten" # May depend on how representation is fed
    hidden_dims: [128, 64]
    activation: "relu"
    use_batch_norm: false
    learning_rate: 0.0003
    # ... other parameters like num_epochs for this MLP
  jepa_reward_mlp:
    enabled: true
    hidden_dims: [128, 64]
    activation: "relu"
    use_batch_norm: false
    learning_rate: 0.0003
    # ...
```
These sections allow enabling the training of separate reward predictors for representations derived from the Encoder-Decoder model and the JEPA model, respectively. The `input_dim` for these MLPs is implicitly the `latent_dim` of the corresponding world model.

The test file `tests/test_reward_prediction.py` demonstrates the instantiation of `RewardPredictorMLP` and the basic training loop structure, confirming the expected shapes for inputs and outputs.

## 3. JEPA Next State Image Reconstruction (via `JEPAStateDecoder`)

While JEPA operates by making predictions in an abstract embedding space, it is valuable to assess what information these embeddings retain about the original visual state. This is achieved by training a separate decoder model, the `JEPAStateDecoder`, to reconstruct the next state image `s_{t+1}` from JEPA's predicted embedding of that state.

### Concept

This evaluation is specific to JEPA because its primary output (the predicted embedding `\hat{z}_{t+1}`) is not directly interpretable as an image. The `JEPAStateDecoder` attempts to invert this embedding back into pixel space, providing a visual proxy for the information content of JEPA's predictive representations.

### The `JEPAStateDecoder` (`src/models/jepa_state_decoder.py`)

-   **Architecture:** The `JEPAStateDecoder` is a Transformer-based decoder, architecturally similar to the decoder component of the Standard Encoder-Decoder model. It uses learnable query tokens and cross-attention mechanisms to synthesize an image.
-   **Input:** Its primary input is the *output of JEPA's predictor module*, which is the predicted embedding of the next state, `\hat{z}_{t+1}` (shape `[batch_size, latent_dim]`). This embedding is projected and used as `memory` for the Transformer decoder.
-   **Objective:** The `JEPAStateDecoder` is trained to reconstruct the ground truth next state image `s_{t+1}` in pixel space.
-   **Loss Function:** The training objective is typically the Mean Squared Error (MSE) between the reconstructed image `\hat{s}_{t+1}` produced by `JEPAStateDecoder` and the actual next state image `s_{t+1}`.
    `L_reconstruction = MSE(JEPAStateDecoder(\hat{z}_{t+1}), s_{t+1})`

### Interpretation

The quality of the reconstructed images offers critical insights into JEPA's learned representations:
-   **High-Fidelity Reconstruction:** If the `JEPAStateDecoder` can produce sharp, accurate reconstructions of `s_{t+1}`, it suggests that JEPA's predictive embedding `\hat{z}_{t+1}` (and by extension, the underlying state representations `z'_t` it was derived from) preserves a significant amount of visual detail from the environment.
-   **Structurally Correct but Abstracted Reconstruction:** If the reconstructions are somewhat blurry or lack fine details but still capture the essential structural elements and objects in the scene, it might indicate that JEPA's embeddings are more abstract, focusing on higher-level features while discarding pixel-level minutiae. This could be advantageous for generalization and computational efficiency.
-   **Poor Reconstruction:** If the decoder fails to produce meaningful images, it could imply that JEPA's predictive embeddings are either too abstract, have lost critical spatial information, or that the predictor itself is not generating sufficiently informative embeddings.

Comparing these reconstructions to those from the main Encoder-Decoder model (which predicts pixels directly) can highlight differences in what each architecture prioritizes.

### Training and Configuration

The `JEPAStateDecoder` is typically trained *after* the main JEPA model has been trained and its weights (online encoder, target encoder, predictor) are frozen. This ensures that the decoder is learning to interpret the fixed embeddings produced by JEPA.
Relevant parameters for training this decoder are found in `config.yaml` under the `jepa_decoder_training` block:
```yaml
jepa_decoder_training:
  enabled: true
  num_epochs: 50
  learning_rate: 0.0003
  checkpoint_path: "best_jepa_decoder.pth"
  validation_plot_dir: "validation_plots/" # For saving sample reconstructions
  # ... other parameters like decoder architecture (depth, heads), batch_size
  early_stopping:
    patience: 10
    delta: 0.001
    metric: "val_loss_jepa_decoder"
```
The architectural parameters for the `JEPAStateDecoder` itself (e.g., `decoder_dim`, `decoder_depth`, `decoder_heads`, `decoder_patch_size`) would also be specified, often mirroring those of the Standard Encoder-Decoder's decoder for comparability.

## 4. Discussion on Evaluation Philosophy

The selection of these two primary evaluation methods—reward prediction and (for JEPA) state reconstruction—provides complementary insights into the quality of learned representations:

-   **Reward Prediction:** Directly assesses the utility of representations for a common downstream task objective in RL. It probes whether task-salient information is encoded.
-   **State Reconstruction (for JEPA):** Investigates the information content of JEPA's abstract embeddings, specifically their ability to retain visual information necessary to reconstruct the original observation. This is crucial for understanding the nature of JEPA's "abstractness."

Together, these evaluations facilitate a nuanced comparison between the direct pixel-prediction strategy of the Encoder-Decoder model and the indirect, embedding-space prediction strategy of JEPA. They help answer whether abstracting away from direct pixel prediction (as in JEPA) leads to representations that are more useful for task-relevant predictions, and what, if any, visual fidelity is lost or preserved in this abstraction. Such comprehensive evaluation is vital for advancing research in world models and for substantiating claims about the representational power of different architectures.
