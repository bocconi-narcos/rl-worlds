# 05 Representation Evaluation

## 1. Introduction to Representation Evaluation

Evaluating the quality of learned representations is paramount in the study of world models. The internal representations, or embeddings, generated by these models are intended to encapsulate the essential information about the environment's state and dynamics. For research aimed at top-tier AI conferences, a rigorous assessment is necessary to determine if these representations are not merely compressive but also capture meaningful, actionable, and task-relevant information. This section details the methodologies employed to assess and compare the representations learned by both the Standard Encoder-Decoder and the Joint Embedding Predictive Architecture (JEPA) models.

## 2. Reward Prediction Task

A primary method for evaluating the utility of learned representations is to assess their effectiveness in predicting task-relevant variables, such as environmental rewards. If a representation can be used to accurately predict rewards, it suggests that it has captured features of the state that are salient for understanding progress and success within the environment, a crucial aspect for downstream reinforcement learning tasks.

### Concept

The core idea is to train a lightweight predictive model—specifically, a Multi-Layer Perceptron (MLP)—on top of the *frozen* latent representations produced by a pre-trained world model. This MLP, termed `RewardPredictorMLP`, attempts to predict the scalar reward `r_t` associated with the state (or state-action pair) from which the representation was derived.

### Implementation Details

-   **Model:** The `RewardPredictorMLP`, defined in `src/models/mlp.py`, is a simple MLP architecture. Its structure (number of hidden layers, dimensions, activation functions, batch normalization) is configurable.
-   **Input Representations:**
    -   For the **Standard Encoder-Decoder model**, the input to the `RewardPredictorMLP` is the latent representation `z_t` produced by its encoder (e.g., ViT, CNN, or MLP encoder) from the current state `s_t`.
    -   For the **JEPA model**, the input is typically the representation generated by its *online encoder* for the current state `s_t` (i.e., `online_encoded_s_t`). Using the online encoder's output reflects the most current understanding of the state by the learnable part of JEPA.
    The world model (Encoder-Decoder or JEPA) is pre-trained and its weights are **frozen** during the training of the `RewardPredictorMLP`. This ensures that the evaluation truly probes the fixed qualities of the learned representation.
-   **Prediction Target:** The `RewardPredictorMLP` is trained to predict the scalar reward `r_t` that was received from the environment when the agent was in state `s_t` (or transitioned from `s_t` by taking `a_t`). The `ExperienceDataset` (from `src/utils/data_utils.py`) provides these (state, action, reward, next_state) tuples.
-   **Training Loop:** The `RewardPredictorMLP` models are trained by the `src/training_engine.py`. This engine calls a dedicated function, `train_reward_mlp_epoch`, located in `src/training_loops/reward_predictor_loop.py`. This loop manages the training epochs, loads the appropriate dataset (containing pre-computed embeddings from the frozen base model and corresponding rewards), performs forward passes through the reward MLP, calculates the loss (typically MSE between predicted reward and actual reward), and executes the optimization step for the reward MLP's parameters.
-   **Loss Function:** The `RewardPredictorMLP` is trained using a Mean Squared Error (MSE) loss.
    `L_reward_pred = MSE(MLP(frozen_representation_s_t), r_t)`

### Interpretation

The performance of the `RewardPredictorMLP` (e.g., low validation MSE) is indicative of the quality of the underlying world model's representations. If the MLP can accurately predict rewards using these representations, it implies that the representations have successfully encoded information critical to understanding the environment's reward structure. This is a strong positive signal for the utility of these representations in subsequent reinforcement learning agents that rely on reward signals.

### Configuration

The configuration for training `RewardPredictorMLP` models is located under the `reward_predictors` key in `config.yaml`. Separate configurations are provided for MLPs trained on Encoder-Decoder outputs (`encoder_decoder_reward_mlp`) and JEPA outputs (`jepa_reward_mlp`).

Key parameters for each predictor include:

-   **`enabled`**: (boolean) If `true`, this specific reward predictor MLP will be trained.
-   **`input_type`**: (string, e.g., `"flatten"`, `"latent"`)
    -   For `encoder_decoder_reward_mlp`: Specifies how the input from the base Encoder-Decoder model's encoder is formed. `"flatten"` might imply flattening the output of a CNN encoder if it's not already a vector. If the encoder output is already a flat vector (e.g., from ViT's CLS token or MLP encoder), this might be implicitly handled or set to something like `"latent"`.
    -   For `jepa_reward_mlp`: The input is typically the `latent_dim` embedding from JEPA's online encoder, so this parameter might be less critical or implicitly `"latent"`.
-   **`hidden_dims`**: (list of integers, e.g., `[256, 128]`) Defines the number of units in each hidden layer of the MLP.
-   **`activation`**: (string, e.g., `"relu"`, `"tanh"`) The activation function used in the hidden layers.
-   **`use_batch_norm`**: (boolean) Whether to include batch normalization layers in the MLP.
-   **`learning_rate`**: (float) The learning rate for optimizing this specific MLP.
-   **`num_epochs`**: (integer) The number of epochs to train this MLP.
-   **`log_interval`**: (integer) Frequency (in batches or epochs) for logging training progress.
-   **`dropout_rate`**: (float, 0.0 to 1.0) Dropout probability applied within the MLP layers.

The `input_dim` for these MLPs is typically derived programmatically by the training script:
-   For `jepa_reward_mlp`, it's usually the `latent_dim` of the JEPA model.
-   For `encoder_decoder_reward_mlp`, it depends on the `encoder_type` and its output shape (e.g., `latent_dim` if the encoder output is a vector, or a calculated flattened dimension if the output is, for instance, a feature map from a CNN that needs flattening based on `input_type`).

Refer to `config.yaml` for default values and further details. The test file `tests/test_reward_prediction.py` also provides context on their usage.

## 3. JEPA Next State Image Reconstruction (via `JEPAStateDecoder`)

While JEPA operates by making predictions in an abstract embedding space, it is valuable to assess what information these embeddings retain about the original visual state. This is achieved by training a separate decoder model, the `JEPAStateDecoder`, to reconstruct the next state image `s_{t+1}` from JEPA's predicted embedding of that state.

### Concept

This evaluation is specific to JEPA because its primary output (the predicted embedding `\hat{z}_{t+1}`) is not directly interpretable as an image. The `JEPAStateDecoder` attempts to invert this embedding back into pixel space, providing a visual proxy for the information content of JEPA's predictive representations.

### The `JEPAStateDecoder` (`src/models/jepa_state_decoder.py`)

-   **Architecture:** The `JEPAStateDecoder` is a Transformer-based decoder, architecturally similar to the decoder component of the Standard Encoder-Decoder model. It uses learnable query tokens and cross-attention mechanisms to synthesize an image.
-   **Input:** Its primary input is the *output of JEPA's predictor module*, which is the predicted embedding of the next state, `\hat{z}_{t+1}` (shape `[batch_size, latent_dim]`). This embedding is projected and used as `memory` for the Transformer decoder.
-   **Objective:** The `JEPAStateDecoder` is trained to reconstruct the ground truth next state image `s_{t+1}` in pixel space.
-   **Loss Function:** The training objective is typically the Mean Squared Error (MSE) between the reconstructed image `\hat{s}_{t+1}` produced by `JEPAStateDecoder` and the actual next state image `s_{t+1}`.
    `L_reconstruction = MSE(JEPAStateDecoder(\hat{z}_{t+1}), s_{t+1})`

### Interpretation

The quality of the reconstructed images offers critical insights into JEPA's learned representations:
-   **High-Fidelity Reconstruction:** If the `JEPAStateDecoder` can produce sharp, accurate reconstructions of `s_{t+1}`, it suggests that JEPA's predictive embedding `\hat{z}_{t+1}` (and by extension, the underlying state representations `z'_t` it was derived from) preserves a significant amount of visual detail from the environment.
-   **Structurally Correct but Abstracted Reconstruction:** If the reconstructions are somewhat blurry or lack fine details but still capture the essential structural elements and objects in the scene, it might indicate that JEPA's embeddings are more abstract, focusing on higher-level features while discarding pixel-level minutiae. This could be advantageous for generalization and computational efficiency.
-   **Poor Reconstruction:** If the decoder fails to produce meaningful images, it could imply that JEPA's predictive embeddings are either too abstract, have lost critical spatial information, or that the predictor itself is not generating sufficiently informative embeddings.

Comparing these reconstructions to those from the main Encoder-Decoder model (which predicts pixels directly) can highlight differences in what each architecture prioritizes.

### Training and Configuration

The `JEPAStateDecoder` is trained *after* the main JEPA model has completed its training (or has been loaded) and its weights (online encoder, target encoder, predictor) are kept frozen. This distinct training phase ensures that the decoder learns to interpret the fixed, pre-learned embeddings from JEPA.

The training process is managed by `src/training_engine.py`, which specifically calls the `train_jepa_state_decoder` function located in `src/training_loops/jepa_decoder_loop.py`. This function handles the epochs, data loading (matching JEPA embeddings to target images), forward passes through the `JEPAStateDecoder`, loss calculation (MSE), and optimization steps for the decoder.

Relevant parameters for configuring the `JEPAStateDecoder` and its training are found in `config.yaml` under the `jepa_decoder_training` block:
```yaml
jepa_decoder_training:
  enabled: true
  num_epochs: 50
  learning_rate: 0.0003
  checkpoint_path: "best_jepa_decoder.pth"
  validation_plot_dir: "validation_plots/" # For saving sample reconstructions
  # ... other parameters like decoder architecture (depth, heads), batch_size
  early_stopping:
    patience: 10
    delta: 0.001
    metric: "val_loss_jepa_decoder"
```
The architectural parameters for the `JEPAStateDecoder` itself (e.g., `decoder_dim`, `decoder_depth`, `decoder_heads`, `decoder_patch_size`) would also be specified, often mirroring those of the Standard Encoder-Decoder's decoder for comparability.

## 4. Discussion on Evaluation Philosophy

The selection of these two primary evaluation methods—reward prediction and (for JEPA) state reconstruction—provides complementary insights into the quality of learned representations:

-   **Reward Prediction:** Directly assesses the utility of representations for a common downstream task objective in RL. It probes whether task-salient information is encoded.
-   **State Reconstruction (for JEPA):** Investigates the information content of JEPA's abstract embeddings, specifically their ability to retain visual information necessary to reconstruct the original observation. This is crucial for understanding the nature of JEPA's "abstractness."

Together, these evaluations facilitate a nuanced comparison between the direct pixel-prediction strategy of the Encoder-Decoder model and the indirect, embedding-space prediction strategy of JEPA. They help answer whether abstracting away from direct pixel prediction (as in JEPA) leads to representations that are more useful for task-relevant predictions, and what, if any, visual fidelity is lost or preserved in this abstraction. Such comprehensive evaluation is vital for advancing research in world models and for substantiating claims about the representational power of different architectures.
