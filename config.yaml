# configs/base_config.yaml

# --- Environment Configuration ---
environment:
  name: "CarRacing-v3" # Example, choose a relevant Gym environment (e.g., "CarRacing-v3", "ALE/Pong-v5")
  # For image-based environments like CarRacing or Atari, ensure appropriate wrappers are used if needed by the env.
  input_channels: 3 # Number of channels in the input image (e.g., 3 for RGB, 1 for grayscale, 4 if using FrameStack)
  image_size: 64    # Target size (height and width) for processed images

# --- Data Collection & Dataset Management ---
data:
  collection:
    num_episodes: 5 # Number of episodes to collect for the dataset
    max_steps_per_episode: 600 # Max steps per episode during data collection
  dataset:
    dir: "datasets/"
    load_path: "" # empty string means don't load, otherwise it's a path relative to dataset.dir
    filename: "car_racing_v3_v2.pkl"
  validation_split: 0.2 # Proportion of data to use for validation (moved from early_stopping)

# --- Model Loading Configuration ---
model_loading:
  dir: "trained_models/"
  load_path: "" # empty string means don't load, otherwise it's a path relative to model_loading.dir
  model_type_to_load: "enc_dec_jepa_style" # options: "std_enc_dec", "jepa", "enc_dec_jepa_style"

# --- General Training Configuration ---
training:
  num_epochs: 50
  batch_size: 64
  learning_rate: 0.003 # General learning rate, can be overridden by specific model configs
  num_workers: 1 # For DataLoader
  log_interval: 50 # Log training progress every N batches
  frame_skipping: 0 # Number of frames to skip. Note: If using action_repetition_k in PPO, this should ideally be 0.
  options:
    skip_std_enc_dec_training_if_loaded: false
    skip_jepa_training_if_loaded: false
  early_stopping: # General early stopping settings; specific metrics/paths are per model type if needed
    patience: 30
    delta: 0.001
    # Metric for Encoder/Decoder, e.g., "val_loss_enc_dec" or "val_accuracy_enc_dec"
    metric_enc_dec: "val_loss_enc_dec"
    checkpoint_path_enc_dec: "best_encoder_decoder.pth" # Relative to model_loading.dir
    # Metric for JEPA, e.g., "val_total_loss_jepa"
    metric_jepa: "val_total_loss_jepa"
    checkpoint_path_jepa: "best_jepa.pth" # Relative to model_loading.dir

# --- Models Configuration ---
models:
  shared_latent_dim: 256 # Output dimension of encoders, input to predictors/decoders
  shared_patch_size: 8   # Global patch_size: Used by ViT encoder, default for decoder_patch_size

  encoder:
    type: "vit"  # Options: "vit", "cnn", "mlp"
    # Specific parameters for the chosen encoder_type.
    # train.py will select the appropriate sub-dictionary based on 'encoder.type'.
    params:
      vit:
        # patch_size for ViT is handled by train.py using models.shared_patch_size
        depth: 8                # Number of Transformer blocks in ViT
        heads: 4                # Number of attention heads in ViT
        mlp_dim: 512            # Dimension of the MLP within ViT Transformer blocks
        pool: 'cls'             # Type of pooling ('cls' token or 'mean' pooling)
        dropout: 0.3            # Dropout rate in ViT
        emb_dropout: 0.25       # Embedding dropout rate in ViT
      cnn:
        num_conv_layers: 3      # Number of convolutional layers
        base_filters: 32        # Number of filters in the first convolutional layer
        kernel_size: 3          # Kernel size for convolutional layers
        stride: 2               # Stride for convolutional layers
        padding: 1              # Padding for convolutional layers
        activation_fn_str: 'relu' # Activation function ('relu' or 'gelu')
        fc_hidden_dim: null     # Dimension of an optional fully connected layer before latent output (null for direct)
        dropout_rate: 0.2       # Dropout rate for CNN encoder
      mlp:
        num_hidden_layers: 2    # Number of hidden layers in the MLP encoder
        hidden_dim: 256         # Dimension of hidden layers in the MLP encoder
        activation_fn_str: 'relu' # Activation function ('relu' or 'gelu')
        dropout_rate: 0.1       # Dropout rate for MLP encoder

  standard_encoder_decoder:
    action_emb_dim: 64      # Dimension for embedding actions
    decoder_dim: 128        # Internal dimension of the Transformer decoder
    decoder_depth: 4        # Number of layers in the Transformer decoder
    decoder_heads: 4        # Number of attention heads in the Transformer decoder
    decoder_mlp_dim: 512    # MLP dimension in the Transformer decoder
    decoder_dropout: 0.2    # Dropout for the decoder
    decoder_patch_size: 8   # Patch size for reconstructing output. If null, defaults to models.shared_patch_size.

  jepa:
    learning_rate: 0.003    # Specific learning rate for JEPA optimizer
    predictor_hidden_dim: 256 # Hidden dimension for the JEPA predictor MLP
    predictor_dropout_rate: 0.3  # Dropout rate for JEPA predictor MLP layers
    ema_decay: 0.99              # EMA decay rate for updating the target encoder in JEPA
    target_encoder_mode: "vjepa2" # Options: "default", "vjepa2", "none"
    # For VICRegLoss coefficients used in JEPA's regularization terms (if not using full auxiliary_loss block for this)
    # vicreg_sim_coeff: 25.0 # (Example, often 0 for JEPA's reg_terms use)
    # vicreg_std_coeff: 25.0
    # vicreg_cov_coeff: 1.0
    decoder_training: # Configuration for the JEPA State Decoder
      enabled: true # Set to true to enable training this decoder
      num_epochs: 50
      learning_rate: 0.0003
      checkpoint_path: "best_jepa_decoder.pth" # Relative to model_loading.dir
      validation_plot_dir: "validation_plots/"
      early_stopping:
        patience: 15
        delta: 0.001
        metric: "val_loss_jepa_decoder" # Example metric

  enc_dec_jepa_style: # Model for fair JEPA comparison using Encoder-Decoder architecture
    # Decoder parameters (decoder_dim, decoder_depth, etc.) are shared with models.standard_encoder_decoder
    predictor_hidden_dim: 256 # Hidden dim for the predictor MLP
    predictor_output_dim: 128 # Output dim of the predictor (should match decoder input dim)
    predictor_dropout_rate: 0.3 # Dropout rate for predictor MLP

  reward_predictors:
    encoder_decoder_reward_mlp:
      enabled: true
      input_type: "flatten" # Assumes flattened decoded image from StandardEncoderDecoder
      hidden_dims: [128, 64]
      num_epochs: 50
      activation: "relu"
      use_batch_norm: false
      learning_rate: 0.0003
      log_interval: 100
      dropout_rate: 0.2
    jepa_reward_mlp:
      enabled: true
      # Input for JEPA's reward MLP typically comes from JEPA's encoder output.
      num_epochs: 200
      hidden_dims: [128, 64]
      activation: "relu"
      use_batch_norm: false
      learning_rate: 0.0003
      log_interval: 100
      dropout_rate: 0.2

  auxiliary_loss: # Configuration for auxiliary losses like VICReg, Barlow Twins, DINO
    type: "vicreg"  # Options: "vicreg", "barlow_twins", "dino"
    weight: 3.0     # General weight for the chosen auxiliary loss
    params:
      vicreg:
        # For JEPA's use of calculate_reg_terms, only std_coeff and cov_coeff are relevant.
        sim_coeff: 0.0  # Default to 0 as per current train.py for reg_terms
        std_coeff: 1.0
        cov_coeff: 1.0
        eps: 0.0001     # Default VICRegLoss epsilon
      barlow_twins:
        lambda_param: 0.0051
        eps: 0.00001
        scale_loss: 1.0
      dino:
        # out_dim for DINOLoss will be set programmatically from model's latent_dim in train.py
        center_ema_decay: 0.9
        eps: 0.00001

# --- PPO Agent Configuration for Data Collection ---
ppo_agent:
  enabled: true # Master switch for using PPO for data collection
  action_repetition_k: 6
  learning_rate: 0.0003
  total_train_timesteps: 3000 # Timesteps to train PPO before data collection
  n_steps: 1024       # PPO n_steps parameter
  batch_size: 64      # PPO batch_size parameter
  n_epochs: 5         # PPO n_epochs parameter
  gamma: 0.99         # PPO gamma parameter
  gae_lambda: 0.95    # PPO gae_lambda parameter
  clip_range: 0.2     # PPO clip_range parameter
  additional_log_std_noise: 0.0
  policy_type: "CnnPolicy" # Policy type, e.g., "CnnPolicy" for image-based envs

# --- Weights & Biases Logging ---
wandb:
  project: "rl_worlds"
  entity: null # Your W&B entity (username or team name)
  run_name_prefix: "exp"
  enabled: true
